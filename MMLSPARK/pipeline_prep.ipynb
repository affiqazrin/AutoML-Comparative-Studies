{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "#from Ipython.core.interactiveShell import InteractiveShell\n",
    "#InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession, SQLContext, Row\n",
    "\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.sql.functions import col, when, explode\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "import pyodbc\n",
    "#from Ipython.display import display, FileLink, FileLinks\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "#pd.option.display.max_columns = None\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\spark\\\\spark-3.0.1-bin-hadoop2.7'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark =SparkSession\\\n",
    "   .builder\\\n",
    "   .appName(\"test\")\\\n",
    "   .enableHiveSupport().getOrCreate()\n",
    "\n",
    "sc= spark.sparkContext\n",
    "sqlContext= SQLContext(sc)\n",
    "\n",
    "findspark.find()\n",
    "\n",
    "#x = np.genfromtxt(\"C:/Users/affiqazrin/Desktop/anacondaScripts/data_berka/trans.asc\", dtype=None, delimiter=\";\")\n",
    "\n",
    "#A = sc.parallelize(x)\n",
    "#df = pd.DataFrame(data=x)\n",
    "#df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sqlContext.read.format(\"csv\")\\\n",
    "   .option(\"header\", \"true\")\\\n",
    "   .load(\"C:/Users/affiqazrin/Desktop/spark_tutorials/Data_FinalProject.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.read_csv(\"C:/Users/affiqazrin/Desktop/spark_tutorials/feature_names.csv\")\n",
    "features=features.transpose()\n",
    "features.column=['fet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: string (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- marital: string (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- default: string (nullable = true)\n",
      " |-- housing: string (nullable = true)\n",
      " |-- loan: string (nullable = true)\n",
      " |-- contact: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- day_of_week: string (nullable = true)\n",
      " |-- duration: string (nullable = true)\n",
      " |-- campaign: string (nullable = true)\n",
      " |-- pdays: string (nullable = true)\n",
      " |-- previous: string (nullable = true)\n",
      " |-- poutcome: string (nullable = true)\n",
      " |-- emp.var.rate: string (nullable = true)\n",
      " |-- cons.price.idx: string (nullable = true)\n",
      " |-- cons.conf.idx: string (nullable = true)\n",
      " |-- euribor3m: string (nullable = true)\n",
      " |-- nr.employed: string (nullable = true)\n",
      " |-- y: string (nullable = true)\n",
      "\n",
      "+---+-----------+--------+-------------------+-------+-------+----+---------+-----+-----------+--------+--------+-----+--------+-----------+------------+--------------+-------------+---------+-----------+---+\n",
      "|age|        job| marital|          education|default|housing|loan|  contact|month|day_of_week|duration|campaign|pdays|previous|   poutcome|emp.var.rate|cons.price.idx|cons.conf.idx|euribor3m|nr.employed|  y|\n",
      "+---+-----------+--------+-------------------+-------+-------+----+---------+-----+-----------+--------+--------+-----+--------+-----------+------------+--------------+-------------+---------+-----------+---+\n",
      "| 56|  housemaid| married|           basic.4y|     no|     no|  no|telephone|  may|        mon|     261|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|       5191| no|\n",
      "| 57|   services| married|        high.school|unknown|     no|  no|telephone|  may|        mon|     149|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|       5191| no|\n",
      "| 37|   services| married|        high.school|     no|    yes|  no|telephone|  may|        mon|     226|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|       5191| no|\n",
      "| 40|     admin.| married|           basic.6y|     no|     no|  no|telephone|  may|        mon|     151|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|       5191| no|\n",
      "| 56|   services| married|        high.school|     no|     no| yes|telephone|  may|        mon|     307|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|       5191| no|\n",
      "| 45|   services| married|           basic.9y|unknown|     no|  no|telephone|  may|        mon|     198|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|       5191| no|\n",
      "| 59|     admin.| married|professional.course|     no|     no|  no|telephone|  may|        mon|     139|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|       5191| no|\n",
      "| 41|blue-collar| married|            unknown|unknown|     no|  no|telephone|  may|        mon|     217|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|       5191| no|\n",
      "| 24| technician|  single|professional.course|     no|    yes|  no|telephone|  may|        mon|     380|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|       5191| no|\n",
      "| 25|   services|  single|        high.school|     no|    yes|  no|telephone|  may|        mon|      50|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|       5191| no|\n",
      "| 41|blue-collar| married|            unknown|unknown|     no|  no|telephone|  may|        mon|      55|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|       5191| no|\n",
      "| 25|   services|  single|        high.school|     no|    yes|  no|telephone|  may|        mon|     222|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|       5191| no|\n",
      "| 29|blue-collar|  single|        high.school|     no|     no| yes|telephone|  may|        mon|     137|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|       5191| no|\n",
      "| 57|  housemaid|divorced|           basic.4y|     no|    yes|  no|telephone|  may|        mon|     293|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|       5191| no|\n",
      "| 35|blue-collar| married|           basic.6y|     no|    yes|  no|telephone|  may|        mon|     146|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|       5191| no|\n",
      "| 54|    retired| married|           basic.9y|unknown|    yes| yes|telephone|  may|        mon|     174|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|       5191| no|\n",
      "| 35|blue-collar| married|           basic.6y|     no|    yes|  no|telephone|  may|        mon|     312|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|       5191| no|\n",
      "| 46|blue-collar| married|           basic.6y|unknown|    yes| yes|telephone|  may|        mon|     440|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|       5191| no|\n",
      "| 50|blue-collar| married|           basic.9y|     no|    yes| yes|telephone|  may|        mon|     353|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|       5191| no|\n",
      "| 39| management|  single|           basic.9y|unknown|     no|  no|telephone|  may|        mon|     195|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|       5191| no|\n",
      "+---+-----------+--------+-------------------+-------+-------+----+---------+-----+-----------+--------+--------+-----+--------+-----------+------------+--------------+-------------+---------+-----------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def spark_shape(self):\n",
    "    return (self.count(), len(self.columns))\n",
    "pyspark.sql.dataframe.DataFrame.shape = spark_shape\n",
    "\n",
    "spark_shape(df)\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "df = sqlContext.createDataFrame([\n",
    "(0, \"a\"),\n",
    "(1, \"b\"),\n",
    "(2, \"c\"),\n",
    "(3, \"a\"),\n",
    "(4, \"a\"),\n",
    "(5, \"c\")\n",
    "], [\"id\", \"category\"])\n",
    "\n",
    "stringIndexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\")\n",
    "model = stringIndexer.fit(df)\n",
    "indexed = model.transform(df)\n",
    "indexed.show()\n",
    "\n",
    "encoder = OneHotEncoder(inputCol=\"categoryIndex\", outputCol=\"vectorIndex\")\n",
    "model = encoder.fit(indexed)\n",
    "encoded = model.transform(indexed)\n",
    "encoded.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clss=df.select('y').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "categorical_cols = [\"job\",\"marital\",\"education\",\"default\",\"housing\",\"loan\",\"contact\",\"month\",\"poutcome\"]\n",
    "categorical_cols2 = [\"job\",\"marital\"]\n",
    "numeric_cols = [\"age\", \"day_of_week\", \"duration\", \"campaign\", \"pdays\",\"previous\"]\n",
    "\n",
    "#df.select(categorical_cols).collect()\n",
    "#df.select(numeric_cols).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cols in categorical_cols:\n",
    "    print(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "for cols in categorical_cols2:\n",
    "    print(cols)\n",
    "    stringIndexer = StringIndexer(\n",
    "        inputCols=[cols + \"_index\" for cols in categorical_cols2], \n",
    "        outputCols=[cols + \"_classVec\" for cols in categorical_cols2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "stages = []\n",
    "\n",
    "#iterate through all categorical values\n",
    "for cols in categorical_cols:\n",
    "    #create a string indexer for those categorical values and assign a new name including the word 'Index'\n",
    "    stringIndexer = StringIndexer(inputCol = cols,\n",
    "                                  outputCol = cols + '_index')\n",
    "\n",
    "    #append the string Indexer to our list of stages\n",
    "    stages += [stringIndexer]\n",
    "    \n",
    "#Create the pipeline. Assign the satges list to the pipeline key word stages\n",
    "pipeline = Pipeline(stages = stages)\n",
    "\n",
    "#fit the pipeline to our dataframe\n",
    "pipelineModel = pipeline.fit(df)\n",
    "\n",
    "#transform the dataframe\n",
    "df= pipelineModel.transform(df)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Feature Types\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "\n",
    "df2 = spark.sql(\"select \\\n",
    "                    cast(id as int) as id, \\\n",
    "                    cast(age as int) as age, \\\n",
    "                    cast(job as string) as job, \\\n",
    "                    cast(marital as string) as marital, \\\n",
    "                    cast(education as string) as education, \\\n",
    "                    cast(default as string) as default, \\\n",
    "                    cast(balance as int) as balance, \\\n",
    "                    cast(housing as string) as housing, \\\n",
    "                    cast(loan as string) as loan, \\\n",
    "                    cast(contact as string) as contact, \\\n",
    "                    cast(day as int) as day, \\\n",
    "                    cast(month as string) as month, \\\n",
    "                    cast(duration as int) as duration, \\\n",
    "                    cast(campaign as int) as campaign, \\\n",
    "                    cast(pdays as int) as pdays, \\\n",
    "                    cast(previous as int) as previous, \\\n",
    "                    cast(poutcome as string) as poutcome, \\\n",
    "                    cast(deposit as string) as deposit \\\n",
    "                from df\")\n",
    "\n",
    "# Data Types\n",
    "df2.dtypes\n",
    "\n",
    "[('id', 'int'),\n",
    " ('age', 'int'),\n",
    " ('job', 'string'),\n",
    " ('marital', 'string'),\n",
    " ('education', 'string'),\n",
    " ('default', 'string'),\n",
    " ('balance', 'int'),\n",
    " ('housing', 'string'),\n",
    " ('loan', 'string'),\n",
    " ('contact', 'string'),\n",
    " ('day', 'int'),\n",
    " ('month', 'string'),\n",
    " ('duration', 'int'),\n",
    " ('campaign', 'int'),\n",
    " ('pdays', 'int'),\n",
    " ('previous', 'int'),\n",
    " ('poutcome', 'string'),\n",
    " ('deposit', 'string')]\n",
    "\n",
    "\n",
    " # Build Pipeline (Error is Here)\n",
    "categorical_cols = [\"job\",\"marital\",\"education\",\"default\",\"housing\",\"loan\",\"contact\",\"month\",\"poutcome\"]\n",
    "numeric_cols = [\"age\", \"balance\", \"day\", \"duration\", \"campaign\", \"pdays\",\"previous\"]\n",
    "\n",
    "stages = []\n",
    "\n",
    "stringIndexer = StringIndexer(inputCol=[cols for cols in categorical_cols],\n",
    "                              outputCol=[cols + \"_index\" for cols in categorical_cols])\n",
    "\n",
    "encoder = OneHotEncoderEstimator(inputCols=[cols + \"_index\" for cols in categorical_cols],\n",
    "                                 outputCols=[cols + \"_classVec\" for cols in categorical_cols])\n",
    "\n",
    "stages += [stringIndexer, encoder]\n",
    "\n",
    "label_string_id = StringIndexer(inputCol=\"deposit\", outputCol=\"label\")\n",
    "stages += [label_string_id]\n",
    "\n",
    "assembler_inputs = [cols + \"_classVec\" for cols in categorical_cols] + numeric_cols\n",
    "assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\")\n",
    "stages += [assembler]\n",
    "\n",
    "# Run Data Through Pipeline\n",
    "pipeline = Pipeline().setStages(stages)\n",
    "pipeline_model = pipeline.fit(df2)\n",
    "prepped_df = pipeline_model.transform(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid param value given for param \"inputCol\". Could not convert <class 'list'> to string type",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\spark\\spark-3.0.1-bin-hadoop2.7\\python\\pyspark\\ml\\param\\__init__.py\u001b[0m in \u001b[0;36m_set\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    446\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 447\u001b[1;33m                     \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtypeConverter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    448\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-3.0.1-bin-hadoop2.7\\python\\pyspark\\ml\\param\\__init__.py\u001b[0m in \u001b[0;36mtoString\u001b[1;34m(value)\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 221\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Could not convert %s to string type\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Could not convert <class 'list'> to string type",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-65e0e75bd82f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mstages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m stringIndexer = StringIndexer(inputCol=[cols for cols in categorical_cols],\n\u001b[0m\u001b[0;32m      7\u001b[0m                               outputCol=[cols + \"_index\" for cols in categorical_cols])\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-3.0.1-bin-hadoop2.7\\python\\pyspark\\__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    108\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Method %s forces keyword arguments.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_input_kwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-3.0.1-bin-hadoop2.7\\python\\pyspark\\ml\\feature.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, inputCol, outputCol, inputCols, outputCols, handleInvalid, stringOrderType)\u001b[0m\n\u001b[0;32m   3622\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_obj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_new_java_obj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"org.apache.spark.ml.feature.StringIndexer\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3623\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_input_kwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3624\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetParams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3625\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3626\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mkeyword_only\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-3.0.1-bin-hadoop2.7\\python\\pyspark\\__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    108\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Method %s forces keyword arguments.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_input_kwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-3.0.1-bin-hadoop2.7\\python\\pyspark\\ml\\feature.py\u001b[0m in \u001b[0;36msetParams\u001b[1;34m(self, inputCol, outputCol, inputCols, outputCols, handleInvalid, stringOrderType)\u001b[0m\n\u001b[0;32m   3634\u001b[0m         \"\"\"\n\u001b[0;32m   3635\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_input_kwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3636\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3637\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3638\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_create_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjava_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-3.0.1-bin-hadoop2.7\\python\\pyspark\\ml\\param\\__init__.py\u001b[0m in \u001b[0;36m_set\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    447\u001b[0m                     \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtypeConverter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Invalid param value given for param \"%s\". %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    450\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_paramMap\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Invalid param value given for param \"inputCol\". Could not convert <class 'list'> to string type"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "stages = []\n",
    "\n",
    "stringIndexer = StringIndexer(inputCol=[cols for cols in categorical_cols],\n",
    "                              outputCol=[cols + \"_index\" for cols in categorical_cols])\n",
    "\n",
    "encoder = OneHotEncoderEstimator(inputCols=[cols + \"_index\" for cols in categorical_cols],\n",
    "                                 outputCols=[cols + \"_classVec\" for cols in categorical_cols])\n",
    "\n",
    "stages += [stringIndexer, encoder]\n",
    "\n",
    "label_string_id = StringIndexer(inputCol=\"deposit\", outputCol=\"label\")\n",
    "stages += [label_string_id]\n",
    "\n",
    "assembler_inputs = [cols + \"_classVec\" for cols in categorical_cols] + numeric_cols\n",
    "assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\")\n",
    "stages += [assembler]\n",
    "\n",
    "# Run Data Through Pipeline\n",
    "pipeline = Pipeline().setStages(stages)\n",
    "pipeline_model = pipeline.fit(df2)\n",
    "prepped_df = pipeline_model.transform(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
