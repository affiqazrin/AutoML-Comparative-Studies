{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import os\n",
    "import time\n",
    "import pyspark\n",
    "import findspark\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "#from pyspark.ml.feature import *\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer, OneHotEncoder, VectorAssembler, IndexToString\n",
    "\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "#from pyspark.sql.functions import *\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col, when, explode\n",
    "from pyspark.sql import SparkSession, SQLContext, Row, HiveContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\spark\\\\spark-3.0.1-bin-hadoop2.7'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findspark.init()\n",
    "\n",
    "# Creatingt Spark SQL environment\n",
    "spark =SparkSession\\\n",
    "   .builder\\\n",
    "   .appName(\"test\")\\\n",
    "   .enableHiveSupport().getOrCreate()\n",
    "\n",
    "sc= spark.sparkContext\n",
    "sqlContext= SQLContext(sc)\n",
    "\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark is an existing SparkSession\n",
    "train = sqlContext.read.format(\"csv\")\\\n",
    "   .option(\"header\", \"true\")\\\n",
    "   .load(\"C:/Users/affiqazrin/Desktop/dataset/titanic/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# String to float on some columns of the dataset : creates a new dataset\n",
    "train = train.select(col(\"Survived\"),\n",
    "                     col(\"Sex\"),\n",
    "                     col(\"Embarked\"),\n",
    "                     col(\"Pclass\").cast(\"float\"),\n",
    "                     col(\"Age\").cast(\"float\"),\n",
    "                     col(\"SibSp\").cast(\"float\"),\n",
    "                     col(\"Fare\").cast(\"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping null values\n",
    "train = train.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spliting in train and test set. Beware : It sorts the dataset\n",
    "(traindf, testdf) = train.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index labels, adding metadata to the label column.\n",
    "# Fit on whole dataset to include all labels in index.\n",
    "genderIndexer = StringIndexer(inputCol=\"Sex\", outputCol=\"indexedSex\")\n",
    "embarkIndexer = StringIndexer(inputCol=\"Embarked\", outputCol=\"indexedEmbarked\")\n",
    "\n",
    "surviveIndexer = StringIndexer(inputCol=\"Survived\", outputCol=\"indexedSurvived\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot Encoder on indexed features\n",
    "genderEncoder = OneHotEncoder(inputCol=\"indexedSex\", outputCol=\"sexVec\")\n",
    "embarkEncoder = OneHotEncoder(inputCol=\"indexedEmbarked\", outputCol=\"embarkedVec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the vector structured data (label,features(vector))\n",
    "assembler = VectorAssembler(inputCols=[\"Pclass\",\n",
    "                                       \"sexVec\",\n",
    "                                       \"Age\",\n",
    "                                       \"SibSp\",\n",
    "                                       \"Fare\",\n",
    "                                       \"embarkedVec\"],outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a RandomForest model.\n",
    "rf = RandomForestClassifier(labelCol=\"indexedSurvived\", featuresCol=\"features\")\n",
    " \n",
    "# Chain indexers and forest in a Pipeline\n",
    "pipeline = Pipeline(stages=[surviveIndexer,\n",
    "                            genderIndexer,\n",
    "                            embarkIndexer,\n",
    "                            genderEncoder,\n",
    "                            embarkEncoder,\n",
    "                            assembler,\n",
    "                            rf]) # genderIndexer,embarkIndexer,genderEncoder,embarkEncoder,\n",
    " \n",
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(traindf)\n",
    " \n",
    "# Predictions\n",
    "predictions = model.transform(testdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+--------------------+\n",
      "|prediction|Survived|            features|\n",
      "+----------+--------+--------------------+\n",
      "|       1.0|       0|[3.0,0.0,18.0,0.0...|\n",
      "|       0.0|       0|(7,[0,2,4],[3.0,3...|\n",
      "|       1.0|       0|[1.0,0.0,2.0,1.0,...|\n",
      "|       1.0|       0|[2.0,0.0,24.0,0.0...|\n",
      "|       1.0|       0|[2.0,0.0,38.0,0.0...|\n",
      "|       1.0|       0|[2.0,0.0,57.0,0.0...|\n",
      "|       0.0|       0|[3.0,0.0,2.0,4.0,...|\n",
      "|       0.0|       0|[3.0,0.0,8.0,3.0,...|\n",
      "|       0.0|       0|[3.0,0.0,10.0,0.0...|\n",
      "|       0.0|       0|[3.0,0.0,11.0,4.0...|\n",
      "|       0.0|       0|[3.0,0.0,18.0,0.0...|\n",
      "|       0.0|       0|[3.0,0.0,18.0,2.0...|\n",
      "|       0.0|       0|[3.0,0.0,20.0,0.0...|\n",
      "|       0.0|       0|[3.0,0.0,20.0,1.0...|\n",
      "|       0.0|       0|[3.0,0.0,22.0,0.0...|\n",
      "|       0.0|       0|[3.0,0.0,25.0,0.0...|\n",
      "|       0.0|       0|[3.0,0.0,25.0,1.0...|\n",
      "|       1.0|       0|[3.0,0.0,26.0,1.0...|\n",
      "|       0.0|       0|[3.0,0.0,30.0,1.0...|\n",
      "|       1.0|       0|[3.0,0.0,31.0,1.0...|\n",
      "+----------+--------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select example rows to display.\n",
    "predictions.columns \n",
    " \n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\",\n",
    "                   \"Survived\",\n",
    "                   \"features\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.toPandas().to_csv('titanic_READY.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.22\n"
     ]
    }
   ],
   "source": [
    "# Select (prediction, true label) and compute test error\n",
    "predictions = predictions.select(col(\"Survived\").cast(\"Float\"),col(\"prediction\"))\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"Survived\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassificationModel: uid=RandomForestClassifier_b9c83511cc69, numTrees=20, numClasses=2, numFeatures=7\n",
      "Accuracy = 0.78\n",
      "f1 = 0.773593\n",
      "weightedPrecision = 0.78431\n",
      "weightedRecall = 0.78\n"
     ]
    }
   ],
   "source": [
    "rfModel = model.stages[6]\n",
    "print(rfModel)  # summary only\n",
    " \n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"Survived\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy = %g\" % accuracy)\n",
    " \n",
    "evaluatorf1 = MulticlassClassificationEvaluator(labelCol=\"Survived\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "f1 = evaluatorf1.evaluate(predictions)\n",
    "print(\"f1 = %g\" % f1)\n",
    " \n",
    "evaluatorwp = MulticlassClassificationEvaluator(labelCol=\"Survived\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
    "wp = evaluatorwp.evaluate(predictions)\n",
    "print(\"weightedPrecision = %g\" % wp)\n",
    " \n",
    "evaluatorwr = MulticlassClassificationEvaluator(labelCol=\"Survived\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
    "wr = evaluatorwr.evaluate(predictions)\n",
    "print(\"weightedRecall = %g\" % wr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close sparkcontext\n",
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
