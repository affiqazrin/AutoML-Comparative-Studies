{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "#from Ipython.core.interactiveShell import InteractiveShell\n",
    "#InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession, SQLContext, Row\n",
    "\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.sql.functions import col, when, explode\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "import pyodbc\n",
    "#from Ipython.display import display, FileLink, FileLinks\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "#pd.option.display.max_columns = None\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark =SparkSession\\\n",
    "   .builder\\\n",
    "   .appName(\"test\")\\\n",
    "   .enableHiveSupport().getOrCreate()\n",
    "\n",
    "sc= spark.sparkContext\n",
    "sqlContext= SQLContext(sc)\n",
    "\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sqlContext.read.format(\"csv\")\\\n",
    "   .option(\"header\", \"true\")\\\n",
    "   .load(\"C:/Users/affiqazrin/Desktop/dataset/Data_FinalProject.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Feature Types\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "\n",
    "df2 = spark.sql(\"select \\\n",
    "                    cast(age as int) as age, \\\n",
    "                    cast(job as string) as job, \\\n",
    "                    cast(marital as string) as marital, \\\n",
    "                    cast(education as string) as education, \\\n",
    "                    cast(default as string) as default, \\\n",
    "                    cast(housing as string) as housing, \\\n",
    "                    cast(loan as string) as loan, \\\n",
    "                    cast(contact as string) as contact, \\\n",
    "                    cast(day_of_week as string) as day, \\\n",
    "                    cast(month as string) as month, \\\n",
    "                    cast(duration as int) as duration, \\\n",
    "                    cast(campaign as int) as campaign, \\\n",
    "                    cast(pdays as int) as pdays, \\\n",
    "                    cast(previous as int) as previous, \\\n",
    "                    cast(poutcome as string) as poutcome, \\\n",
    "                    cast(y as string) as deposit \\\n",
    "                from df\")\n",
    "\n",
    "# Data Types\n",
    "df2.dtypes\n",
    "[('age', 'int'),\n",
    " ('job', 'string'),\n",
    " ('marital', 'string'),\n",
    " ('education', 'string'),\n",
    " ('default', 'string'),\n",
    " ('housing', 'string'),\n",
    " ('loan', 'string'),\n",
    " ('contact', 'string'),\n",
    " ('day', 'string'),\n",
    " ('month', 'string'),\n",
    " ('duration', 'int'),\n",
    " ('campaign', 'int'),\n",
    " ('pdays', 'int'),\n",
    " ('previous', 'int'),\n",
    " ('poutcome', 'string'),\n",
    " ('deposit', 'string')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#target (y=deposit)\n",
    "df.groupBy(\"y\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#categorical attrib(11)\n",
    "categorical_cols = [\"job\",\n",
    "                    \"marital\",\n",
    "                    \"education\",\n",
    "                    \"default\",\n",
    "                    \"housing\",\n",
    "                    \"loan\",\n",
    "                    \"contact\",\n",
    "                    \"day\",\n",
    "                    \"month\",\n",
    "                    \"poutcome\",\n",
    "                    \"deposit\"]\n",
    " \n",
    "#numerical attrib(5)\n",
    "numeric_cols = [\"age\",\n",
    "                \"duration\",\n",
    "                \"campaign\",\n",
    "                \"pdays\",\n",
    "                \"previous\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "stages = []\n",
    "\n",
    "#iterate through all categorical values\n",
    "for cols in categorical_cols:\n",
    "    #create a string indexer for those categorical values and assign a new name including the word 'Index'\n",
    "    stringIndexer = StringIndexer(inputCol = cols,\n",
    "                                  outputCol = cols + '_index')\n",
    "\n",
    "    oneHotEncoder = OneHotEncoder(inputCol = cols + '_index',\n",
    "                                  outputCol = cols + '_classVec').setDropLast(False)\n",
    "\n",
    "    #append the string Indexer to our list of stages\n",
    "    stages += [stringIndexer, oneHotEncoder]\n",
    "    \n",
    "assembler_inputs = [cols + \"_classVec\" for cols in categorical_cols] + numeric_cols\n",
    "\n",
    "assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\")\n",
    "stages += [assembler]\n",
    "    \n",
    "#Create the pipeline. Assign the satges list to the pipeline key word stages\n",
    "pipeline = Pipeline(stages = stages)\n",
    "\n",
    "#fit the pipeline to our dataframe\n",
    "pipelineModel = pipeline.fit(df2)\n",
    "\n",
    "#transform the dataframe\n",
    "df3= pipelineModel.transform(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.toPandas().to_csv('Data_FinalProject_READY.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
