{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## H2O Automl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform, time, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import itertools\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from tabulate import tabulate\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import accuracy_score, auc, classification_report, confusion_matrix, f1_score\n",
    "from sklearn.metrics import log_loss, mean_squared_error, precision_score, recall_score, r2_score\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import h2o\n",
    "from h2o.automl import H2OAutoML\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Operating system version....', platform.platform())\n",
    "print(\"Python version is........... %s.%s.%s\" % sys.version_info[:3])\n",
    "print('pandas version is...........', pd.__version__)\n",
    "print('numpy version is............', np.__version__)\n",
    "print('matplotlib version is.......', matplotlib.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the h2o server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = int(time.time())\n",
    "\n",
    "localH2O = h2o.init(ip = \"localhost\",\n",
    "                    port = 54321,\n",
    "                    max_mem_size=\"24G\",\n",
    "                    nthreads = 6)\n",
    "h2o.no_progress()\n",
    "h2o.remove_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a correlation plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Correlation_plot(df_new, TARGET_COL):\n",
    "    plt.ioff()\n",
    "    red_green = [\"#ff0000\", \"#00ff00\"]\n",
    "    sns.set_palette(red_green)\n",
    "    np.seterr(divide='ignore', invalid='ignore')\n",
    "    g = sns.pairplot(df_new,\n",
    "                     diag_kind='kde',\n",
    "                     hue=TARGET_COL, \n",
    "                     markers=[\"o\", \"D\"],\n",
    "                     size=1.5,\n",
    "                     aspect=1,\n",
    "                     plot_kws={\"s\": 10})\n",
    "    g.fig.subplots_adjust(right=0.9)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a method to get the data set, and split it into train, validate, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Model_Data():\n",
    "\n",
    "    path = \"C:/Users/affiqazrin/Desktop/mmlspark/Data_FinalProject_READY2.csv\"\n",
    "    model = pd.read_csv(path)\n",
    "    \n",
    "    feature_columns = [\"duration\",\n",
    "                       \"campaign\",\n",
    "                       \"pdays\",\n",
    "                       \"previous\"]\n",
    "    response_column = [\"indexedDeposit\"]\n",
    "    mask = feature_columns + response_column\n",
    "    \n",
    "    # Correlation_plot(model[mask], response_column)\n",
    "    \n",
    "    X = model[feature_columns].values\n",
    "    y = model[response_column].values.ravel()\n",
    "    \n",
    "    sm = SMOTE(random_state=12)\n",
    "    X_resampled, y_resampled = sm.fit_sample(X, y)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_resampled,\n",
    "                                                        y_resampled,\n",
    "                                                        test_size = 0.3,\n",
    "                                                        random_state = 0)\n",
    "    # reshape so we can append the columns\n",
    "    y_train_ = y_train.reshape(y_train.shape[0], 1)\n",
    "    df_train = pd.DataFrame(X_train, columns=feature_columns)\n",
    "    df_train[response_column] = pd.DataFrame(y_train_).astype(int)\n",
    "    print('train: \\n', df_train.head(5))\n",
    "    \n",
    "    y_test_ = y_test.reshape(y_test.shape[0], 1)\n",
    "    df_test = pd.DataFrame(X_test, columns=feature_columns)\n",
    "    df_test[response_column] = pd.DataFrame(y_test_).astype(int)\n",
    "    print('test: \\n',  df_test.head(5))\n",
    "    \n",
    "    train = h2o.H2OFrame(df_train)\n",
    "    train[response_column] = train[response_column].asfactor()\n",
    "    X_train = train[feature_columns]\n",
    "    y_train = train[response_column]\n",
    "    \n",
    "    test = h2o.H2OFrame(df_test)\n",
    "    test[response_column] = test[response_column].asfactor()\n",
    "    X_test = test[feature_columns]\n",
    "    y_test = test[response_column]\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, train, test, feature_columns, response_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_LOCAL_PATH = \"C:/Users/affiqazrin/Desktop/mmlspark/Data_FinalProject_READY4.csv\"\n",
    "df = pd.read_csv(DATASET_LOCAL_PATH)\n",
    "    \n",
    "ALL_COLS = [\"age\", #numerical\n",
    "            \"job\", #categorical\n",
    "            \"marital\", #categorical\n",
    "            \"education\", #categorical\n",
    "            \"default\", #categorical\n",
    "            \"housing\", #categorical, binary\n",
    "            \"loan\", #categorical, binary\n",
    "            \"contact\", #categorical\n",
    "            \"day\", #categorical\n",
    "            \"month\", #categorical\n",
    "            \"duration\", #numerical\n",
    "            \"campaign\", #categorical\n",
    "            \"pdays\", #numerical\n",
    "            \"previous\", #numerical\n",
    "            \"poutcome\", #categorical\n",
    "            \"deposit\", #categorical, binary\n",
    "           ]\n",
    "    \n",
    "NUMERICAL_COLS = [\"age\", #numerical\n",
    "                  \"duration\", #numerical\n",
    "                  \"pdays\", #numerical\n",
    "                  \"previous\", #numerical\n",
    "                 ]\n",
    "    \n",
    "CATEGORICAL_COLS = [\"job\", #categorical\n",
    "                    \"marital\", #categorical\n",
    "                    \"education\", #categorical\n",
    "                    \"default\", #categorical\n",
    "                    \"housing\", #categorical, binary\n",
    "                    \"loan\", #categorical, binary\n",
    "                    \"contact\", #categorical\n",
    "                    \"day\", #categorical\n",
    "                    \"month\", #categorical\n",
    "                    \"campaign\", #categorical\n",
    "                    \"poutcome\" #categorical\n",
    "                   ]\n",
    "\n",
    "TARGET_COL = [\"deposit\" #categorical, binary\n",
    "             ]\n",
    "    \n",
    "le = LabelEncoder()\n",
    "#TARGET_COL2 = le.fit_transform(df[TARGET_COL])\n",
    "TARGET_COL2 = df[TARGET_COL].apply(LabelEncoder().fit_transform)\n",
    "    \n",
    "ohe = OneHotEncoder(handle_unknown='ignore')\n",
    "CATEGORICAL_COLS2 = pd.DataFrame(ohe.fit_transform(df[CATEGORICAL_COLS]).toarray())\n",
    "    \n",
    "df[DATA_PROCESSED] = pd.concat([df[NUMERICAL_COLS], CATEGORICAL_COLS2], axis=1)\n",
    "mask = DATA_PROCESSED + TARGET_COL2\n",
    "    \n",
    "# Correlation_plot(model[mask], TARGET_COL)    \n",
    "X = df[DATA_PROCESSED].values\n",
    "y = df[TARGET_COL2].values.ravel()\n",
    "    \n",
    "sm = SMOTE(random_state=12)\n",
    "X_resampled, y_resampled = sm.fit_sample(X, y)\n",
    "    \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled,\n",
    "                                                        y_resampled,\n",
    "                                                        test_size = 0.3,\n",
    "                                                        random_state = 0)\n",
    "    \n",
    "print('Size of resampled data:')\n",
    "print(' train shape... ', X_train.shape, y_train.shape)\n",
    "print(' test shape.... ', X_test.shape, y_test.shape)\n",
    "    \n",
    "DATA_PROCESSED_HEADER=list(df[DATA_PROCESSED].columns.values)\n",
    "n_features = len(DATA_PROCESSED_HEADER)\n",
    "n_ALL_COLS = len(ALL_COLS)\n",
    "n_NUMERICAL_COLS = len(NUMERICAL_COLS)\n",
    "n_CATEGORICAL_COLS = len(CATEGORICAL_COLS)\n",
    "\n",
    "# reshape so we can append the columns\n",
    "y_train_ = y_train.reshape(y_train.shape[0], 1)\n",
    "df_train = pd.DataFrame(X_train, columns=DATA_PROCESSED_HEADER)\n",
    "df_train[TARGET_COL] = pd.DataFrame(y_train_).astype(int)\n",
    "#print('train: \\n', df_train.head(5))\n",
    "\n",
    "train = h2o.H2OFrame(df_train)\n",
    "train[TARGET_COL] = train[TARGET_COL].asfactor()\n",
    "X_train = train[DATA_PROCESSED]\n",
    "y_train = train[TARGET_COL]\n",
    "    \n",
    "y_test_ = y_test.reshape(y_test.shape[0], 1)\n",
    "df_test = pd.DataFrame(X_test, columns=DATA_PROCESSED_HEADER)\n",
    "df_test[TARGET_COL] = pd.DataFrame(y_test_).astype(int)\n",
    "#print('test: \\n',  df_test.head(5))\n",
    "      \n",
    "test = h2o.H2OFrame(df_test)\n",
    "test[TARGET_COL] = test[TARGET_COL].asfactor()\n",
    "X_test = test[DATA_PROCESSED]\n",
    "y_test = test[TARGET_COL]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ROC_Curve(best_model, train):\n",
    "    performance = best_model.model_performance(train)\n",
    "    auc = performance.auc()\n",
    "    false_positive_rate = performance.fprs\n",
    "    true_positive_rate = performance.tprs\n",
    "\n",
    "    plt.style.use('ggplot')\n",
    "    plt.figure()\n",
    "    plt.plot(false_positive_rate, true_positive_rate, 'k--')\n",
    "    plt.plot(false_positive_rate, \n",
    "             true_positive_rate, \n",
    "             color='darkorange',\n",
    "             lw = 2,\n",
    "             label='ROC curve (area = %0.2f)' % auc)\n",
    "    plt.plot([0,1], [0,1], color = 'navy', lw = 2, linestyle = '--')\n",
    "    plt.xlim(0, 0.2)\n",
    "    plt.ylim(0.8, 1)\n",
    "    plt.xlabel('False positive rate')\n",
    "    plt.ylabel('True positive rate')\n",
    "    plt.title('ROC curve')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the performance of each predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Plot_predictor_importance(best_model):\n",
    "    fig, ax = plt.subplots()\n",
    "    variables = best_model._model_json['output']['variable_importances']['variable']\n",
    "    y_pos = np.arange(len(variables))\n",
    "    scaled_importance = best_model._model_json['output']['variable_importances']['scaled_importance']\n",
    "    ax.barh(y_pos, \n",
    "            scaled_importance, \n",
    "            align='center', \n",
    "            color='green', \n",
    "            ecolor='black', \n",
    "            height=0.5)\n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels(variables)\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_xlabel('Scaled Importance')\n",
    "    ax.set_title('Variable Importance')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Plot_Confusion_Matrix(best_model):\n",
    "    y_predicted = best_model.predict(X_test)\n",
    "    y_predicted_ = y_predicted.as_data_frame(use_pandas=True, header=False)\n",
    "    y_pred = y_predicted_['predict']\n",
    "    \n",
    "    y_test_ = y_test.as_data_frame(use_pandas=True, header=False)\n",
    "    y = y_test_[TARGET_COL]\n",
    "\n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "    \n",
    "    cmap = plt.cm.Blues\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    title='Confusion matrix (on test data)'\n",
    "    classes = [0, 1]\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=0)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "    c_report = classification_report(y, y_pred)\n",
    "    print('\\nClassification report:\\n', c_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the model metrics for the validate and test data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Print_Metrics(best_model, X_test, y_test):\n",
    "    print('\\nModel performance on test data set:')\n",
    "    \n",
    "    predictions = best_model.predict(X_test)\n",
    "    misclassification_rate = (predictions['predict'] == y_test).as_data_frame(use_pandas=True).mean()\n",
    "    \n",
    "    performance = best_model.model_performance(test)\n",
    "    accuracy  = performance.accuracy()[0][1]\n",
    "    precision = performance.precision()[0][1]\n",
    "    recall    = 0.0  #performance.recall()[0][1]  not available yet\n",
    "    F1        = performance.F1()[0][1]\n",
    "    r2        = performance.r2()\n",
    "    auc       = performance.auc()\n",
    "    mse       = performance.mse()\n",
    "    logloss   = performance.logloss()\n",
    "    \n",
    "    header = [\"Metric\", \"Test dataset\"]\n",
    "    table = [[\"accuracy\",               accuracy],\n",
    "             [\"precision\",              precision],\n",
    "             [\"recall\",                 recall],\n",
    "             [\"misclassification rate\", misclassification_rate],\n",
    "             [\"F1\",                     F1],\n",
    "             [\"r2\",                     r2],\n",
    "             [\"AUC\",                    auc],\n",
    "             [\"mse\",                    mse],\n",
    "             [\"logloss\",                logloss]\n",
    "            ]\n",
    "    \n",
    "    print(tabulate(table, header, tablefmt=\"fancy_grid\", floatfmt=\".8f\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Bottle Rocket dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " X_train, y_train, X_test, y_test, train, test, DATA_PROCESSED, TARGET_COL = Get_Model_Data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the AutoML model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Run():\n",
    "    aml = H2OAutoML(max_models = 500, \n",
    "                    seed = 7, \n",
    "                    max_runtime_secs = 1*60*60, \n",
    "                    nfolds = 5, \n",
    "                    stopping_metric = \"misclassification\",\n",
    "                    project_name=\"bottle_rocket\",\n",
    "                    stopping_rounds = 5)\n",
    "    \n",
    "    aml.train(x = DATA_PROCESSED, \n",
    "              y = TARGET_COL,\n",
    "              training_frame = train)\n",
    "\n",
    "    lb = aml.leaderboard\n",
    "    return lb, X_test, y_test, train, test, DATA_PROCESSED, TARGET_COL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let’s run the model and get the best model from the leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = Run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose the best model from the leader board.\n",
    "#### At this time, we cannot choose a Stacked Ensemble model because it lacks the metrics we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = lb[0][0]\n",
    "print('df:\\n', df)\n",
    "df2 = df.as_data_frame(use_pandas=True, header=True)\n",
    "\n",
    "for row in range(len(lb)):\n",
    "    model_id = df2.iloc[row,0]\n",
    "    print('model_id:', model_id)\n",
    "    if 'StackedEnsemble' in model_id:\n",
    "        continue\n",
    "    else:\n",
    "        best_model = h2o.get_model(model_id)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROC_Curve(best_model, train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot_Confusion_Matrix(best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = h2o.save_model(model=best_model, path=\"h2o_automl\", force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model = h2o.load_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot predictor importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot_predictor_importance(saved_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the performance of the model\n",
    "#### Note that ‘recall’ is not available in this version of h2o.automl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Print_Metrics(saved_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the computation time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = int(time.time())\n",
    "d = divmod(end_time - start_time,86400)  # days\n",
    "h = divmod(d[1],3600)  # hours\n",
    "m = divmod(h[1],60)  # minutes\n",
    "s = m[1]  # seconds\n",
    "print('%d days, %d hours, %d minutes, %d seconds' % (d[0],h[0],m[0],s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "leaderboard = lb.getLeaderboard(\"ALL\")\n",
    "leaderboard.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
